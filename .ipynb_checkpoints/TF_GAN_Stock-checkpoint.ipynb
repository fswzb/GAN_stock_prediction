{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "southern-pioneer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gan in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (2.1.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow-gan) (0.12.0)\n",
      "Requirement already satisfied: tensorflow-probability>=0.7 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow-gan) (0.15.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow-hub>=0.2->tensorflow-gan) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/yutong/Library/Python/3.8/lib/python/site-packages (from tensorflow-hub>=0.2->tensorflow-gan) (1.22.2)\n",
      "Requirement already satisfied: six>=1.9 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from protobuf>=3.8.0->tensorflow-hub>=0.2->tensorflow-gan) (1.15.0)\n",
      "Requirement already satisfied: decorator in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (4.4.2)\n",
      "Requirement already satisfied: absl-py in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (0.11.0)\n",
      "Requirement already satisfied: dm-tree in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (0.1.6)\n",
      "Requirement already satisfied: gast>=0.3.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (0.3.3)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (2.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Check that imports for the rest of the file work.\n",
    "import tensorflow.compat.v1 as tf\n",
    "!pip install tensorflow-gan\n",
    "import tensorflow_gan as tfgan\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Allow matplotlib images to render immediately.\n",
    "%matplotlib inline\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)  # Disable noisy outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "tough-poetry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4366.64</td>\n",
       "      <td>4411.01</td>\n",
       "      <td>4287.11</td>\n",
       "      <td>4356.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4356.32</td>\n",
       "      <td>4417.35</td>\n",
       "      <td>4222.62</td>\n",
       "      <td>4410.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4471.38</td>\n",
       "      <td>4494.52</td>\n",
       "      <td>4395.34</td>\n",
       "      <td>4397.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4547.35</td>\n",
       "      <td>4602.11</td>\n",
       "      <td>4477.95</td>\n",
       "      <td>4482.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4588.03</td>\n",
       "      <td>4611.55</td>\n",
       "      <td>4530.20</td>\n",
       "      <td>4532.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Open     High      Low    Close\n",
       "1  4366.64  4411.01  4287.11  4356.45\n",
       "2  4356.32  4417.35  4222.62  4410.13\n",
       "3  4471.38  4494.52  4395.34  4397.94\n",
       "4  4547.35  4602.11  4477.95  4482.73\n",
       "5  4588.03  4611.55  4530.20  4532.76"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SP500_average = pd.read_csv(\n",
    "    \"./data_stock/SP500_average.csv\",\n",
    "    ).drop([0]).drop(columns=['Date'])\n",
    "SP500_average.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "consistent-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(mode, params):\n",
    "  assert 'batch_size' in params\n",
    "  assert 'noise_dims' in params\n",
    "  bs = params['batch_size']\n",
    "  nd = params['noise_dims']\n",
    "  split = 'train' if mode == tf.estimator.ModeKeys.TRAIN else 'test'\n",
    "  shuffle = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "  just_noise = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "  \n",
    "  noise_ds = (tf.data.Dataset.from_tensors(0).repeat()\n",
    "              .map(lambda _: tf.random.normal([bs, nd])))\n",
    "  if just_noise:\n",
    "    return noise_ds\n",
    "  SP500_average = pd.read_csv(\n",
    "    \"./data_stock/SP500_average.csv\",\n",
    "    ).drop([0]).drop(columns=['Date'])\n",
    "  SP500_average_ds = tf.data.Dataset.from_tensor_slices(SP500_average.values.tolist()).cache()\n",
    "  train_size = tf.data.experimental.cardinality(SP500_average_ds)\n",
    "    \n",
    "  if split == 'train':\n",
    "    SP500_average_ds.take(train_size)\n",
    "  else:\n",
    "    SP500_average_ds.skip(train_size)\n",
    "    \n",
    "  if shuffle:\n",
    "    SP500_average_ds = SP500_average_ds.shuffle(\n",
    "        buffer_size=10000, reshuffle_each_iteration=True)\n",
    "  SP500_average_ds = (SP500_average_ds.batch(bs, drop_remainder=True)\n",
    "               .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "  return tf.data.Dataset.zip((noise_ds, SP500_average_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "imported-offense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3356.04 3363.29 3335.44 3360.47]\n",
      " [3367.27 3409.57 3367.27 3408.63]\n",
      " [3579.31 3581.23 3556.85 3557.54]\n",
      " [4367.43 4416.75 4367.43 4395.64]\n",
      " [4210.34 4238.04 4201.64 4232.6 ]\n",
      " [3352.7  3368.95 3310.47 3340.97]\n",
      " [3791.84 3843.09 3791.84 3826.31]\n",
      " [4319.57 4365.57 4290.49 4363.55]\n",
      " [4089.95 4098.19 4082.54 4097.17]\n",
      " [3411.23 3428.92 3384.45 3385.49]\n",
      " [3363.56 3402.93 3363.56 3383.54]\n",
      " [3226.14 3278.7  3209.45 3246.59]\n",
      " [3385.87 3397.18 3361.39 3380.8 ]\n",
      " [4484.4  4492.81 4482.28 4486.23]\n",
      " [3885.55 3902.92 3874.71 3876.5 ]\n",
      " [4204.78 4204.78 4164.4  4166.45]\n",
      " [3323.17 3351.03 3318.14 3349.16]\n",
      " [3438.5  3460.53 3415.34 3453.49]\n",
      " [3515.47 3527.94 3480.55 3488.67]\n",
      " [3138.7  3154.9  3127.12 3131.29]\n",
      " [4206.14 4218.78 4176.81 4211.47]\n",
      " [2514.92 2538.18 2459.96 2488.65]\n",
      " [3793.58 3851.69 3730.19 3841.94]\n",
      " [4594.96 4651.14 4583.16 4649.23]\n",
      " [4497.34 4520.4  4496.41 4519.63]\n",
      " [3288.26 3302.73 3284.53 3294.61]\n",
      " [3668.28 3682.73 3657.17 3666.72]\n",
      " [3114.4  3115.01 3032.13 3050.33]\n",
      " [4513.76 4537.36 4513.76 4528.79]\n",
      " [4794.23 4808.93 4775.33 4778.73]\n",
      " [4775.21 4786.83 4765.75 4766.18]\n",
      " [3464.9  3466.46 3440.45 3465.39]\n",
      " [2865.86 2874.14 2793.15 2820.  ]\n",
      " [3408.74 3431.56 3354.54 3360.95]\n",
      " [3370.34 3381.01 3326.44 3333.69]\n",
      " [2842.43 2879.22 2830.88 2874.56]\n",
      " [4351.01 4361.88 4329.79 4358.13]\n",
      " [3342.48 3342.48 3268.89 3271.03]\n",
      " [4138.78 4194.17 4138.78 4180.17]\n",
      " [3434.28 3447.28 3428.15 3446.83]\n",
      " [3509.73 3514.77 3493.25 3500.31]\n",
      " [3698.08 3698.26 3676.16 3687.26]\n",
      " [4728.59 4748.83 4706.71 4726.35]\n",
      " [3046.61 3068.67 3023.4  3029.73]\n",
      " [2555.87 2615.91 2520.02 2541.47]\n",
      " [3781.88 3804.53 3780.37 3798.91]\n",
      " [2805.1  2851.85 2805.1  2846.06]\n",
      " [2290.71 2300.73 2191.86 2237.4 ]\n",
      " [3407.73 3419.48 3389.25 3401.2 ]\n",
      " [4572.87 4608.08 4567.59 4605.38]\n",
      " [3105.92 3128.44 3101.17 3115.86]\n",
      " [4168.61 4188.72 4151.72 4155.86]\n",
      " [2953.63 2980.29 2953.63 2971.61]\n",
      " [4141.58 4151.69 4120.87 4124.66]\n",
      " [4402.95 4415.47 4387.01 4400.64]\n",
      " [3851.68 3859.23 3797.16 3855.36]\n",
      " [3163.84 3211.72 3163.84 3193.93]\n",
      " [4326.6  4355.43 4326.6  4352.34]\n",
      " [4274.45 4286.12 4271.16 4280.7 ]\n",
      " [3910.49 3918.35 3902.64 3911.23]\n",
      " [4652.5  4666.7  4600.22 4620.64]\n",
      " [3213.42 3223.27 3181.49 3190.14]\n",
      " [3073.2  3073.73 3004.63 3009.05]\n",
      " [3992.78 4020.63 3992.78 4019.87]\n",
      " [4374.45 4394.87 4347.96 4354.19]\n",
      " [4712.   4743.83 4682.17 4682.94]\n",
      " [4474.81 4492.99 4445.7  4468.73]\n",
      " [4372.41 4386.68 4364.03 4384.63]\n",
      " [4602.82 4652.94 4510.27 4513.04]\n",
      " [4479.33 4485.68 4435.46 4443.05]\n",
      " [4662.93 4683.   4662.59 4680.06]\n",
      " [2436.5  2453.57 2280.52 2398.1 ]\n",
      " [4096.11 4129.48 4095.51 4128.8 ]\n",
      " [3166.44 3184.15 3142.93 3145.32]\n",
      " [3176.17 3179.78 3115.7  3152.05]\n",
      " [3705.98 3712.39 3660.54 3672.82]\n",
      " [2393.48 2466.97 2319.78 2409.39]\n",
      " [4406.51 4412.02 4386.22 4391.34]\n",
      " [4632.24 4632.24 4568.7  4577.11]\n",
      " [3350.92 3357.92 3327.54 3335.47]\n",
      " [3418.09 3432.09 3413.13 3431.28]\n",
      " [3336.25 3389.49 3336.25 3369.16]\n",
      " [2776.99 2818.57 2762.36 2789.82]\n",
      " [4589.49 4608.03 4495.12 4538.43]\n",
      " [3333.9  3360.74 3332.91 3351.6 ]\n",
      " [4493.75 4495.9  4468.99 4470.  ]\n",
      " [3018.59 3053.89 2999.74 3053.24]\n",
      " [4804.51 4818.62 4774.27 4793.54]\n",
      " [4098.45 4116.93 4061.41 4115.68]\n",
      " [3696.25 3711.27 3688.57 3701.17]\n",
      " [4429.07 4440.82 4429.07 4436.52]\n",
      " [2930.91 2930.91 2892.47 2912.43]\n",
      " [2918.46 2954.86 2912.16 2939.51]\n",
      " [3842.51 3914.5  3842.51 3901.82]\n",
      " [2799.34 2806.51 2764.32 2799.55]\n",
      " [3296.2  3330.14 3279.74 3310.24]\n",
      " [3357.38 3362.27 3292.4  3319.47]\n",
      " [3851.93 3903.76 3851.93 3875.44]\n",
      " [3285.57 3285.57 3229.1  3281.06]\n",
      " [3485.14 3501.38 3468.35 3484.55]]\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset\n",
    "params = {'batch_size': 100, 'noise_dims':64}\n",
    "with tf.Graph().as_default():\n",
    "  ds = input_fn(tf.estimator.ModeKeys.TRAIN, params)\n",
    "  example = next(iter(tfds.as_numpy(ds)))[1]\n",
    "  print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "careful-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dense(inputs, units, l2_weight):\n",
    "  return tf.layers.dense(\n",
    "      inputs, units, None,\n",
    "      kernel_initializer=tf.keras.initializers.glorot_uniform,\n",
    "      kernel_regularizer=tf.keras.regularizers.l2(l=l2_weight),\n",
    "      bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))\n",
    "\n",
    "def _dense_sigmoid(inputs, units, l2_weight):\n",
    "  return tf.layers.dense(\n",
    "      inputs, units, tf.nn.sigmoid,\n",
    "      kernel_initializer=tf.keras.initializers.glorot_uniform,\n",
    "      kernel_regularizer=tf.keras.regularizers.l2(l=l2_weight),\n",
    "      bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))\n",
    "\n",
    "def _batch_norm(inputs, is_training):\n",
    "  return tf.layers.batch_normalization(\n",
    "      inputs, momentum=0.999, epsilon=0.001, training=is_training)\n",
    "\n",
    "def _deconv2d(inputs, filters, kernel_size, stride, l2_weight):\n",
    "  return tf.layers.conv2d_transpose(\n",
    "      inputs, filters, [kernel_size, kernel_size], strides=[stride, stride], \n",
    "      activation=tf.nn.relu, padding='same',\n",
    "      kernel_initializer=tf.keras.initializers.glorot_uniform,\n",
    "      kernel_regularizer=tf.keras.regularizers.l2(l=l2_weight),\n",
    "      bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))\n",
    "\n",
    "def _conv2d(inputs, filters, kernel_size, stride, l2_weight):\n",
    "  return tf.layers.conv2d(\n",
    "      inputs, filters, [kernel_size, kernel_size], strides=[stride, stride], \n",
    "      activation=None, padding='same',\n",
    "      kernel_initializer=tf.keras.initializers.glorot_uniform,\n",
    "      kernel_regularizer=tf.keras.regularizers.l2(l=l2_weight),\n",
    "      bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "developing-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unconditional_generator(noise, mode, weight_decay=2.5e-5):\n",
    "  \"\"\"Generator to produce unconditional MNIST images.\"\"\"\n",
    "  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "  \n",
    "  net = _dense(noise, 100, weight_decay)\n",
    "  net = _leaky_relu(net)\n",
    "  \n",
    "  net = _dense(net, 72, weight_decay)\n",
    "  net = _leaky_relu(net)\n",
    "  \n",
    "  net = _dense(net, 10, weight_decay)\n",
    "  net = _leaky_relu(net)\n",
    "  \n",
    "  net = _dense(net, 4, weight_decay)\n",
    "\n",
    "  return net\n",
    "\n",
    "  return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "vocational-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "_leaky_relu = lambda net: tf.nn.leaky_relu(net, alpha=0.01)\n",
    "\n",
    "def unconditional_discriminator(price, unused_conditioning, mode, weight_decay=2.5e-5):\n",
    "  del unused_conditioning\n",
    "  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "  \n",
    "  net = _dense(price, 100, weight_decay)\n",
    "  net = _leaky_relu(net)\n",
    "  \n",
    "  net = _dense(net, 72, weight_decay)\n",
    "  net = _leaky_relu(net)\n",
    "  \n",
    "  net = _dense(net, 10, weight_decay)\n",
    "  net = _leaky_relu(net)\n",
    "  \n",
    "  net = _dense(net, 1, weight_decay)\n",
    "\n",
    "  return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bottom-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_gan.examples.mnist import util as eval_util\n",
    "import os\n",
    "\n",
    "def get_eval_metric_ops_fn(gan_model):\n",
    "  real_data_logits = tf.reduce_mean(gan_model.discriminator_real_outputs)\n",
    "  gen_data_logits = tf.reduce_mean(gan_model.discriminator_gen_outputs)\n",
    "  return {\n",
    "      'real_data_logits': tf.metrics.mean(real_data_logits),\n",
    "      'gen_data_logits': tf.metrics.mean(gen_data_logits),\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "illegal-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 32 #@param\n",
    "noise_dimensions = 4 #@param\n",
    "generator_lr = 0.001 #@param\n",
    "discriminator_lr = 0.0002 #@param\n",
    "\n",
    "def gen_opt():\n",
    "  gstep = tf.train.get_or_create_global_step()\n",
    "  base_lr = generator_lr\n",
    "  # Halve the learning rate at 1000 steps.\n",
    "  lr = tf.cond(gstep < 1000, lambda: base_lr, lambda: base_lr / 2.0)\n",
    "  return tf.train.AdamOptimizer(lr, 0.5)\n",
    "\n",
    "gan_estimator = tfgan.estimator.GANEstimator(\n",
    "    generator_fn=unconditional_generator,\n",
    "    discriminator_fn=unconditional_discriminator,\n",
    "    generator_loss_fn=tfgan.losses.wasserstein_generator_loss,\n",
    "    discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,\n",
    "    params={'batch_size': train_batch_size, 'noise_dims': noise_dimensions},\n",
    "    generator_optimizer=gen_opt,\n",
    "    discriminator_optimizer=tf.train.AdamOptimizer(discriminator_lr, 0.5),\n",
    "    get_eval_metric_ops_fn=get_eval_metric_ops_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-inside",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-208-5a6d8c6fd259>:2: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  return tf.layers.dense(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time since start: 0.30 min\n",
      "Trained from step 0 to 500 in 28.23 steps / sec\n",
      "Average discriminator output on Real: 1086.13  Fake: 54.31\n",
      "[   6.4926496  149.41077    155.02751   -294.27905  ]\n",
      "[ 184.20216    30.018309   19.314964 -213.68317 ]\n",
      "[ 184.07515    16.113003  234.30624  -428.79193 ]\n",
      "[ 239.70973    49.634525  107.92092  -347.82047 ]\n",
      "[ 1.5318539e+02  3.3687103e-01  1.5161713e+02 -4.4768878e+02]\n",
      "[  60.782314   39.98801    77.09361  -270.02844 ]\n",
      "[ 107.667595   93.3926     78.99993  -187.05069 ]\n",
      "[ 297.35608   173.09541    48.247345 -152.95764 ]\n",
      "[ 170.81488   -10.375096  167.47704  -264.1674  ]\n",
      "[ 283.8185   184.78732  241.46326 -384.43173]\n",
      "[  63.405647   92.677734  153.28928  -310.16168 ]\n",
      "[  69.933205  155.91492   147.35551  -429.38675 ]\n",
      "[  42.098965  114.72638   101.60146  -266.4379  ]\n",
      "[ 17.741762   2.827221  26.945404 -40.912533]\n",
      "[  76.97097  117.63283  190.93979 -392.07483]\n",
      "[ 134.8925     -13.9446535  125.74613   -327.77008  ]\n",
      "[  78.72778    59.41582   109.83474  -103.793304]\n",
      "[   6.1242604   70.152626   160.83536   -144.72188  ]\n",
      "[ 144.85356     3.140689  188.03578  -274.87903 ]\n",
      "[ 133.78606   114.16062    99.713554 -159.24889 ]\n",
      "Time since start: 0.69 min\n",
      "Trained from step 500 to 1000 in 35.34 steps / sec\n",
      "Average discriminator output on Real: 1371.22  Fake: 93.89\n",
      "[ 465.44427  227.06456  255.79332 -460.97684]\n",
      "[  99.97641  218.09212  268.19037 -644.3541 ]\n",
      "[ 280.2976  152.6575  345.1293 -870.5878]\n",
      "[ 760.1849   313.96457  369.24615 -634.9083 ]\n",
      "[ 108.068      42.531513  128.91068  -315.1651  ]\n",
      "[ 166.87695    -3.412566  145.01698  -399.5462  ]\n",
      "[ 249.07555  115.46724  249.48372 -615.0047 ]\n",
      "[ 217.53159  226.34674  222.17119 -475.46024]\n",
      "[ -20.879433  267.77142   114.23077  -623.34576 ]\n",
      "[ 164.9438   201.49484  314.92352 -730.3927 ]\n",
      "[ 244.79022   41.50636  240.2879  -525.1681 ]\n",
      "[ 178.4244    73.23519  201.31558 -489.8117 ]\n",
      "[ 155.93062  262.31104  319.5979  -701.81995]\n",
      "[  48.552315  147.23102   129.92473  -352.8268  ]\n",
      "[ 197.69113   80.14876   67.49093 -210.00743]\n",
      "[ 297.63745  191.12758  276.77023 -699.8035 ]\n",
      "[  97.62809   92.14099  156.79913 -389.69745]\n",
      "[ 239.14328  128.64061  191.17725 -369.36945]\n",
      "[ 178.47852   78.78984   96.09194 -340.6408 ]\n",
      "[ 162.97699   -15.802228  146.06589  -339.08942 ]\n"
     ]
    }
   ],
   "source": [
    "# Disable noisy output.\n",
    "tf.autograph.set_verbosity(0, False)\n",
    "\n",
    "import time\n",
    "steps_per_eval = 500 #@param\n",
    "max_train_steps = 5000 #@param\n",
    "batches_for_eval_metrics = 100 #@param\n",
    "\n",
    "# Used to track metrics.\n",
    "steps = []\n",
    "real_logits, fake_logits = [], []\n",
    "real_mnist_scores, mnist_scores, frechet_distances = [], [], []\n",
    "\n",
    "cur_step = 0\n",
    "start_time = time.time()\n",
    "while cur_step < max_train_steps:\n",
    "  next_step = min(cur_step + steps_per_eval, max_train_steps)\n",
    "\n",
    "  start = time.time()\n",
    "  gan_estimator.train(input_fn, max_steps=next_step)\n",
    "  steps_taken = next_step - cur_step\n",
    "  time_taken = time.time() - start\n",
    "  print('Time since start: %.2f min' % ((time.time() - start_time) / 60.0))\n",
    "  print('Trained from step %i to %i in %.2f steps / sec' % (\n",
    "      cur_step, next_step, steps_taken / time_taken))\n",
    "  cur_step = next_step\n",
    "  \n",
    "  # Calculate some metrics.\n",
    "  metrics = gan_estimator.evaluate(input_fn, steps=batches_for_eval_metrics)\n",
    "  steps.append(cur_step)\n",
    "  real_logits.append(metrics['real_data_logits'])\n",
    "  fake_logits.append(metrics['gen_data_logits'])\n",
    "  print('Average discriminator output on Real: %.2f  Fake: %.2f' % (\n",
    "      real_logits[-1], fake_logits[-1]))\n",
    "  \n",
    "  # Vizualize some images.\n",
    "  iterator = gan_estimator.predict(\n",
    "      input_fn, hooks=[tf.train.StopAtStepHook(num_steps=21)])\n",
    "  try:\n",
    "    daily_prices = np.array([next(iterator) for _ in range(20)])\n",
    "  except StopIteration:\n",
    "    pass\n",
    "  for p in daily_prices:\n",
    "    print(p)\n",
    "  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
