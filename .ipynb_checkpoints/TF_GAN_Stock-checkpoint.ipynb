{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "southern-pioneer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gan in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (2.1.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow-gan) (0.12.0)\n",
      "Requirement already satisfied: tensorflow-probability>=0.7 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow-gan) (0.15.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow-hub>=0.2->tensorflow-gan) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/yutong/Library/Python/3.8/lib/python/site-packages (from tensorflow-hub>=0.2->tensorflow-gan) (1.22.2)\n",
      "Requirement already satisfied: six>=1.9 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from protobuf>=3.8.0->tensorflow-hub>=0.2->tensorflow-gan) (1.15.0)\n",
      "Requirement already satisfied: decorator in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (4.4.2)\n",
      "Requirement already satisfied: absl-py in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (0.11.0)\n",
      "Requirement already satisfied: dm-tree in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (0.1.6)\n",
      "Requirement already satisfied: gast>=0.3.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (0.3.3)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from tensorflow-probability>=0.7->tensorflow-gan) (2.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Check that imports for the rest of the file work.\n",
    "import tensorflow.compat.v1 as tf\n",
    "!pip install tensorflow-gan\n",
    "import tensorflow_gan as tfgan\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Allow matplotlib images to render immediately.\n",
    "%matplotlib inline\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)  # Disable noisy outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "tough-poetry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4366.64</td>\n",
       "      <td>4411.01</td>\n",
       "      <td>4287.11</td>\n",
       "      <td>4356.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4356.32</td>\n",
       "      <td>4417.35</td>\n",
       "      <td>4222.62</td>\n",
       "      <td>4410.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4471.38</td>\n",
       "      <td>4494.52</td>\n",
       "      <td>4395.34</td>\n",
       "      <td>4397.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4547.35</td>\n",
       "      <td>4602.11</td>\n",
       "      <td>4477.95</td>\n",
       "      <td>4482.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4588.03</td>\n",
       "      <td>4611.55</td>\n",
       "      <td>4530.20</td>\n",
       "      <td>4532.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Open     High      Low    Close\n",
       "1  4366.64  4411.01  4287.11  4356.45\n",
       "2  4356.32  4417.35  4222.62  4410.13\n",
       "3  4471.38  4494.52  4395.34  4397.94\n",
       "4  4547.35  4602.11  4477.95  4482.73\n",
       "5  4588.03  4611.55  4530.20  4532.76"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SP500_average = pd.read_csv(\n",
    "    \"./data_stock/SP500_average.csv\",\n",
    "    ).drop([0]).drop(columns=['Date'])\n",
    "SP500_average.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "consistent-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(mode, params):\n",
    "  assert 'batch_size' in params\n",
    "  assert 'noise_dims' in params\n",
    "  bs = params['batch_size']\n",
    "  nd = params['noise_dims']\n",
    "  split = 'train' if mode == tf.estimator.ModeKeys.TRAIN else 'test'\n",
    "  shuffle = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "  just_noise = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "  \n",
    "  noise_ds = (tf.data.Dataset.from_tensors(0).repeat()\n",
    "              .map(lambda _: tf.random.normal([bs, nd])))\n",
    "  if just_noise:\n",
    "    return noise_ds\n",
    "  SP500_average = pd.read_csv(\n",
    "    \"./data_stock/SP500_average.csv\",\n",
    "    ).drop([0]).drop(columns=['Date'])\n",
    "  SP500_average_ds = tf.data.Dataset.from_tensor_slices(SP500_average.values.tolist()).cache()\n",
    "  train_size = tf.data.experimental.cardinality(SP500_average_ds)\n",
    "    \n",
    "  if split == 'train':\n",
    "    SP500_average_ds.take(train_size)\n",
    "  else:\n",
    "    SP500_average_ds.skip(train_size)\n",
    "    \n",
    "  if shuffle:\n",
    "    SP500_average_ds = SP500_average_ds.shuffle(\n",
    "        buffer_size=10000, reshuffle_each_iteration=True)\n",
    "  SP500_average_ds = (SP500_average_ds.batch(bs, drop_remainder=True)\n",
    "               .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "  return tf.data.Dataset.zip((noise_ds, SP500_average_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "imported-offense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3356.04 3363.29 3335.44 3360.47]\n",
      " [3367.27 3409.57 3367.27 3408.63]\n",
      " [3579.31 3581.23 3556.85 3557.54]\n",
      " [4367.43 4416.75 4367.43 4395.64]\n",
      " [4210.34 4238.04 4201.64 4232.6 ]\n",
      " [3352.7  3368.95 3310.47 3340.97]\n",
      " [3791.84 3843.09 3791.84 3826.31]\n",
      " [4319.57 4365.57 4290.49 4363.55]\n",
      " [4089.95 4098.19 4082.54 4097.17]\n",
      " [3411.23 3428.92 3384.45 3385.49]\n",
      " [3363.56 3402.93 3363.56 3383.54]\n",
      " [3226.14 3278.7  3209.45 3246.59]\n",
      " [3385.87 3397.18 3361.39 3380.8 ]\n",
      " [4484.4  4492.81 4482.28 4486.23]\n",
      " [3885.55 3902.92 3874.71 3876.5 ]\n",
      " [4204.78 4204.78 4164.4  4166.45]\n",
      " [3323.17 3351.03 3318.14 3349.16]\n",
      " [3438.5  3460.53 3415.34 3453.49]\n",
      " [3515.47 3527.94 3480.55 3488.67]\n",
      " [3138.7  3154.9  3127.12 3131.29]\n",
      " [4206.14 4218.78 4176.81 4211.47]\n",
      " [2514.92 2538.18 2459.96 2488.65]\n",
      " [3793.58 3851.69 3730.19 3841.94]\n",
      " [4594.96 4651.14 4583.16 4649.23]\n",
      " [4497.34 4520.4  4496.41 4519.63]\n",
      " [3288.26 3302.73 3284.53 3294.61]\n",
      " [3668.28 3682.73 3657.17 3666.72]\n",
      " [3114.4  3115.01 3032.13 3050.33]\n",
      " [4513.76 4537.36 4513.76 4528.79]\n",
      " [4794.23 4808.93 4775.33 4778.73]\n",
      " [4775.21 4786.83 4765.75 4766.18]\n",
      " [3464.9  3466.46 3440.45 3465.39]\n",
      " [2865.86 2874.14 2793.15 2820.  ]\n",
      " [3408.74 3431.56 3354.54 3360.95]\n",
      " [3370.34 3381.01 3326.44 3333.69]\n",
      " [2842.43 2879.22 2830.88 2874.56]\n",
      " [4351.01 4361.88 4329.79 4358.13]\n",
      " [3342.48 3342.48 3268.89 3271.03]\n",
      " [4138.78 4194.17 4138.78 4180.17]\n",
      " [3434.28 3447.28 3428.15 3446.83]\n",
      " [3509.73 3514.77 3493.25 3500.31]\n",
      " [3698.08 3698.26 3676.16 3687.26]\n",
      " [4728.59 4748.83 4706.71 4726.35]\n",
      " [3046.61 3068.67 3023.4  3029.73]\n",
      " [2555.87 2615.91 2520.02 2541.47]\n",
      " [3781.88 3804.53 3780.37 3798.91]\n",
      " [2805.1  2851.85 2805.1  2846.06]\n",
      " [2290.71 2300.73 2191.86 2237.4 ]\n",
      " [3407.73 3419.48 3389.25 3401.2 ]\n",
      " [4572.87 4608.08 4567.59 4605.38]\n",
      " [3105.92 3128.44 3101.17 3115.86]\n",
      " [4168.61 4188.72 4151.72 4155.86]\n",
      " [2953.63 2980.29 2953.63 2971.61]\n",
      " [4141.58 4151.69 4120.87 4124.66]\n",
      " [4402.95 4415.47 4387.01 4400.64]\n",
      " [3851.68 3859.23 3797.16 3855.36]\n",
      " [3163.84 3211.72 3163.84 3193.93]\n",
      " [4326.6  4355.43 4326.6  4352.34]\n",
      " [4274.45 4286.12 4271.16 4280.7 ]\n",
      " [3910.49 3918.35 3902.64 3911.23]\n",
      " [4652.5  4666.7  4600.22 4620.64]\n",
      " [3213.42 3223.27 3181.49 3190.14]\n",
      " [3073.2  3073.73 3004.63 3009.05]\n",
      " [3992.78 4020.63 3992.78 4019.87]\n",
      " [4374.45 4394.87 4347.96 4354.19]\n",
      " [4712.   4743.83 4682.17 4682.94]\n",
      " [4474.81 4492.99 4445.7  4468.73]\n",
      " [4372.41 4386.68 4364.03 4384.63]\n",
      " [4602.82 4652.94 4510.27 4513.04]\n",
      " [4479.33 4485.68 4435.46 4443.05]\n",
      " [4662.93 4683.   4662.59 4680.06]\n",
      " [2436.5  2453.57 2280.52 2398.1 ]\n",
      " [4096.11 4129.48 4095.51 4128.8 ]\n",
      " [3166.44 3184.15 3142.93 3145.32]\n",
      " [3176.17 3179.78 3115.7  3152.05]\n",
      " [3705.98 3712.39 3660.54 3672.82]\n",
      " [2393.48 2466.97 2319.78 2409.39]\n",
      " [4406.51 4412.02 4386.22 4391.34]\n",
      " [4632.24 4632.24 4568.7  4577.11]\n",
      " [3350.92 3357.92 3327.54 3335.47]\n",
      " [3418.09 3432.09 3413.13 3431.28]\n",
      " [3336.25 3389.49 3336.25 3369.16]\n",
      " [2776.99 2818.57 2762.36 2789.82]\n",
      " [4589.49 4608.03 4495.12 4538.43]\n",
      " [3333.9  3360.74 3332.91 3351.6 ]\n",
      " [4493.75 4495.9  4468.99 4470.  ]\n",
      " [3018.59 3053.89 2999.74 3053.24]\n",
      " [4804.51 4818.62 4774.27 4793.54]\n",
      " [4098.45 4116.93 4061.41 4115.68]\n",
      " [3696.25 3711.27 3688.57 3701.17]\n",
      " [4429.07 4440.82 4429.07 4436.52]\n",
      " [2930.91 2930.91 2892.47 2912.43]\n",
      " [2918.46 2954.86 2912.16 2939.51]\n",
      " [3842.51 3914.5  3842.51 3901.82]\n",
      " [2799.34 2806.51 2764.32 2799.55]\n",
      " [3296.2  3330.14 3279.74 3310.24]\n",
      " [3357.38 3362.27 3292.4  3319.47]\n",
      " [3851.93 3903.76 3851.93 3875.44]\n",
      " [3285.57 3285.57 3229.1  3281.06]\n",
      " [3485.14 3501.38 3468.35 3484.55]]\n"
     ]
    }
   ],
   "source": [
    "# sanity check dataset\n",
    "params = {'batch_size': 100, 'noise_dims':64}\n",
    "with tf.Graph().as_default():\n",
    "  ds = input_fn(tf.estimator.ModeKeys.TRAIN, params)\n",
    "  example = next(iter(tfds.as_numpy(ds)))[1]\n",
    "  print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "careful-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dense(inputs, units, l2_weight, activation = None):\n",
    "  return tf.layers.dense(\n",
    "      inputs, units, activation,\n",
    "      kernel_initializer=tf.keras.initializers.glorot_uniform,\n",
    "      kernel_regularizer=tf.keras.regularizers.l2(l=l2_weight),\n",
    "      bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))\n",
    "\n",
    "\n",
    "def _batch_norm(inputs, is_training):\n",
    "  return tf.layers.batch_normalization(\n",
    "      inputs, momentum=0.999, epsilon=0.001, training=is_training)\n",
    "\n",
    "def _deconv2d(inputs, filters, kernel_size, stride, l2_weight):\n",
    "  return tf.layers.conv2d_transpose(\n",
    "      inputs, filters, [kernel_size, kernel_size], strides=[stride, stride], \n",
    "      activation=tf.nn.relu, padding='same',\n",
    "      kernel_initializer=tf.keras.initializers.glorot_uniform,\n",
    "      kernel_regularizer=tf.keras.regularizers.l2(l=l2_weight),\n",
    "      bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))\n",
    "\n",
    "def _conv2d(inputs, filters, kernel_size, stride, l2_weight):\n",
    "  return tf.layers.conv2d(\n",
    "      inputs, filters, [kernel_size, kernel_size], strides=[stride, stride], \n",
    "      activation=None, padding='same',\n",
    "      kernel_initializer=tf.keras.initializers.glorot_uniform,\n",
    "      kernel_regularizer=tf.keras.regularizers.l2(l=l2_weight),\n",
    "      bias_regularizer=tf.keras.regularizers.l2(l=l2_weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "suited-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "_leaky_relu = lambda net: tf.nn.leaky_relu(net, alpha=0.01)\n",
    "def unconditional_generator(noise, mode, weight_decay=2.5e-5):\n",
    "  \"\"\"Generator to produce unconditional MNIST images.\"\"\"\n",
    "  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "  \n",
    "  net = _dense(noise, 100, weight_decay)\n",
    "  net = _leaky_relu(net)\n",
    "  \n",
    "  net = _dense(net, 72, weight_decay)\n",
    "  net = _leaky_relu(net)\n",
    "  \n",
    "  net = _dense(net, 10, weight_decay)\n",
    "  net = _leaky_relu(net)\n",
    "  \n",
    "  net = _dense(net, 4, weight_decay)\n",
    "\n",
    "  return net\n",
    "\n",
    "  return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "drawn-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "_leaky_relu = lambda net: tf.nn.leaky_relu(net, alpha=0.01)\n",
    "\n",
    "def unconditional_discriminator(price, unused_conditioning, mode, weight_decay=2.5e-5):\n",
    "  del unused_conditioning\n",
    "  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "  \n",
    "  net = _dense(price, 100, weight_decay)\n",
    "  net = _leaky_relu(net)\n",
    "  \n",
    "  net = _dense(net, 72, weight_decay)\n",
    "  net = _leaky_relu(net)\n",
    "  \n",
    "  net = _dense(net, 10, weight_decay)\n",
    "  net = _leaky_relu(net)\n",
    "  \n",
    "  net = _dense(net, 1, weight_decay)\n",
    "\n",
    "  return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "approved-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_gan.examples.mnist import util as eval_util\n",
    "import os\n",
    "\n",
    "def get_eval_metric_ops_fn(gan_model):\n",
    "  real_data_logits = tf.reduce_mean(gan_model.discriminator_real_outputs)\n",
    "  gen_data_logits = tf.reduce_mean(gan_model.discriminator_gen_outputs)\n",
    "  return {\n",
    "      'real_data_logits': tf.metrics.mean(real_data_logits),\n",
    "      'gen_data_logits': tf.metrics.mean(gen_data_logits),\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "speaking-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 32 #@param\n",
    "noise_dimensions = 4 #@param\n",
    "generator_lr = 0.001 #@param\n",
    "discriminator_lr = 0.0002 #@param\n",
    "\n",
    "def gen_opt():\n",
    "  gstep = tf.train.get_or_create_global_step()\n",
    "  base_lr = generator_lr\n",
    "  # Halve the learning rate at 1000 steps.\n",
    "  lr = tf.cond(gstep < 1000, lambda: base_lr, lambda: base_lr / 2.0)\n",
    "  return tf.train.AdamOptimizer(lr, 0.5)\n",
    "\n",
    "gan_estimator = tfgan.estimator.GANEstimator(\n",
    "    generator_fn=unconditional_generator,\n",
    "    discriminator_fn=unconditional_discriminator,\n",
    "    generator_loss_fn=tfgan.losses.wasserstein_generator_loss,\n",
    "    discriminator_loss_fn=tfgan.losses.wasserstein_discriminator_loss,\n",
    "    params={'batch_size': train_batch_size, 'noise_dims': noise_dimensions},\n",
    "    generator_optimizer=gen_opt,\n",
    "    discriminator_optimizer=tf.train.AdamOptimizer(discriminator_lr, 0.5),\n",
    "    get_eval_metric_ops_fn=get_eval_metric_ops_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-specialist",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-223-7179cacbd0c8>:2: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  return tf.layers.dense(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time since start: 0.31 min\n",
      "Trained from step 0 to 500 in 27.04 steps / sec\n",
      "Average discriminator output on Real: 35.53  Fake: -2.58\n",
      "[157.38638   0.        0.       36.34581]\n",
      "[477.3972    0.      103.67341   0.     ]\n",
      "[138.61774   97.49743  118.352715   0.      ]\n",
      "[71.225426 66.24938  56.443935  0.      ]\n",
      "[81.986206 17.145731 40.20009  35.10775 ]\n",
      "[202.13907    0.         4.455717   0.      ]\n",
      "[176.367   117.98594  62.08183 111.94539]\n",
      "[178.68259 219.7834  142.48547  27.05446]\n",
      "[93.73706    0.         4.2511845  0.       ]\n",
      "[8.663336 0.       0.       0.      ]\n",
      "[537.38074 251.89174 434.4584    0.     ]\n",
      "[ 97.63781   0.        0.      112.44525]\n",
      "[0. 0. 0. 0.]\n",
      "[212.50275   0.        0.      153.45932]\n",
      "[363.11032 105.4425  266.40903   0.     ]\n",
      "[187.9487     0.         0.        50.539394]\n",
      "[176.0686   0.     111.0014   0.    ]\n",
      "[167.73021    0.         0.        49.169132]\n",
      "[511.03528   0.        0.        0.     ]\n",
      "[0. 0. 0. 0.]\n",
      "Time since start: 0.84 min\n",
      "Trained from step 500 to 1000 in 33.74 steps / sec\n",
      "Average discriminator output on Real: 109.92  Fake: -2.31\n",
      "[473.78433    0.        35.907814   0.      ]\n",
      "[310.58536 230.3197  217.57195   0.     ]\n",
      "[255.19424   0.       59.20033   0.     ]\n",
      "[0. 0. 0. 0.]\n",
      "[147.38689   40.200176   0.         0.      ]\n",
      "[266.98953   41.824745 113.80791    0.      ]\n",
      "[262.70084   0.       72.71821   0.     ]\n",
      "[265.17642   0.        0.        0.     ]\n",
      "[129.14828     0.          2.6000633   0.       ]\n",
      "[177.19807    0.        42.458153   0.      ]\n",
      "[319.6042   0.       0.       0.    ]\n",
      "[ 83.67893 115.72505  72.21262  30.32651]\n",
      "[303.764    0.      77.1255   0.    ]\n",
      "[121.87525  26.73352 156.99681   0.     ]\n",
      "[365.19547    0.        41.429874   0.      ]\n",
      "[154.57278  16.75167   0.        0.     ]\n",
      "[134.28787    0.         0.        36.810017]\n",
      "[52.63778  28.906967 55.615364  0.      ]\n",
      "[266.45758 140.50494 107.52113   0.     ]\n",
      "[0. 0. 0. 0.]\n",
      "Time since start: 1.20 min\n",
      "Trained from step 1000 to 1500 in 34.66 steps / sec\n",
      "Average discriminator output on Real: 186.97  Fake: 2.31\n",
      "[320.07977   0.      384.5248    0.     ]\n",
      "[54.268314 23.508806  0.        0.      ]\n",
      "[247.42389   0.      125.01417   0.     ]\n",
      "[250.25916 154.17036 204.3021    0.     ]\n",
      "[264.1128  215.49753 288.11716   0.     ]\n",
      "[104.332     0.      152.88092   0.     ]\n",
      "[154.20631 211.1671  202.76384   0.     ]\n",
      "[301.15668 252.41881 329.00867   0.     ]\n",
      "[170.83905 155.52124 202.70854   0.     ]\n",
      "[367.74036    0.        86.288025   0.      ]\n",
      "[175.18896 184.56808 244.17781   0.     ]\n",
      "[95.679474  0.        0.        0.      ]\n",
      "[ 52.17065    0.       101.210846   0.      ]\n",
      "[0. 0. 0. 0.]\n",
      "[323.4775  434.13776 394.64157   0.     ]\n",
      "[188.5272  132.20518 245.74281   0.     ]\n",
      "[267.73816 367.3741  369.5975    0.     ]\n",
      "[368.4899   0.     256.0203   0.    ]\n",
      "[191.15848   0.      104.49501   0.     ]\n",
      "[81.13932 55.24714 82.71893  0.     ]\n",
      "Time since start: 1.79 min\n",
      "Trained from step 1500 to 2000 in 20.20 steps / sec\n",
      "Average discriminator output on Real: 239.88  Fake: 8.43\n",
      "[413.0931   77.93641 459.58517   0.     ]\n",
      "[374.32193   51.774635 507.10696    0.      ]\n",
      "[164.24554   0.      268.06906   0.     ]\n",
      "[261.52026 212.60002 377.73016   0.     ]\n",
      "[345.41306 191.41566 423.16696   0.     ]\n",
      "[583.8669  288.47055 715.9985    0.     ]\n",
      "[446.07944 192.19035 616.1234    0.     ]\n",
      "[ 83.815895   0.       149.2534     0.      ]\n",
      "[151.95612 146.07117 194.4684    0.     ]\n",
      "[201.09093  24.75183 283.70596   0.     ]\n",
      "[457.2562  251.64096 562.635     0.     ]\n",
      "[361.75058 252.579   496.9429    0.     ]\n",
      "[249.99118 290.7842  344.19202   0.     ]\n",
      "[166.34941  79.58713 168.50172   0.     ]\n",
      "[259.1252  178.26945 370.6002    0.     ]\n",
      "[58.859684 57.596855 85.21904   0.      ]\n",
      "[263.05402   0.      336.21442   0.     ]\n",
      "[132.25778 117.8307  127.74317   0.     ]\n",
      "[301.69315 218.94746 432.35623   0.     ]\n",
      "[295.53586 228.56126 414.99234   0.     ]\n",
      "Time since start: 2.55 min\n",
      "Trained from step 2000 to 2500 in 22.09 steps / sec\n",
      "Average discriminator output on Real: 279.88  Fake: 13.94\n",
      "[378.39948 204.27153 546.65845   0.     ]\n",
      "[718.6876  310.31042 992.4947    0.     ]\n",
      "[528.1124  412.1461  805.37537   0.     ]\n",
      "[398.48203 269.83026 590.92725   0.     ]\n",
      "[433.23526 340.8366  651.227     0.     ]\n",
      "[325.09085 236.08202 489.50607   0.     ]\n",
      "[414.57675 302.9551  576.48615   0.     ]\n",
      "[177.47195 146.30788 259.28516   0.     ]\n",
      "[217.46698 111.89339 312.52133   0.     ]\n",
      "[215.16467 124.69269 312.673     0.     ]\n",
      "[385.11496 233.58456 541.4586    0.     ]\n",
      "[265.60593 287.1077  462.93878   0.     ]\n",
      "[268.07538 237.95805 417.1962    0.     ]\n",
      "[178.5906   126.595695 239.96805    0.      ]\n",
      "[352.87997 289.01495 531.93036   0.     ]\n",
      "[445.44705 288.9044  663.9061    0.     ]\n",
      "[486.5402  354.5553  742.77985   0.     ]\n",
      "[396.3575  215.01216 593.3845    0.     ]\n",
      "[418.69498 286.95676 631.94354   0.     ]\n",
      "[522.25903 259.7045  766.5354    0.     ]\n",
      "Time since start: 3.22 min\n",
      "Trained from step 2500 to 3000 in 16.01 steps / sec\n",
      "Average discriminator output on Real: 321.58  Fake: 14.90\n",
      "[503.00433 313.61368 792.5337    0.     ]\n",
      "[465.80493 269.3556  731.7179    0.     ]\n",
      "[436.57147 270.99573 694.7222    0.     ]\n",
      "[357.08487 248.27351 539.92725   0.     ]\n",
      "[562.48895 294.5056  845.13715   0.     ]\n",
      "[615.54785 340.93707 892.5205    0.     ]\n",
      "[ 690.5494  461.0236 1096.7034    0.    ]\n",
      "[388.54034 296.4133  612.17834   0.     ]\n",
      "[ 846.5171   620.66675 1399.4832     0.     ]\n",
      "[ 716.52875  367.75925 1128.4165     0.     ]\n",
      "[ 607.0234   555.45764 1105.2078     0.     ]\n",
      "[313.9037  209.71118 485.88312   0.     ]\n",
      "[ 803.895   496.4601 1205.3428    0.    ]\n",
      "[281.74713 188.87228 447.58148   0.     ]\n",
      "[ 702.31287  400.41644 1102.1981     0.     ]\n",
      "[394.85614 231.70332 608.2054    0.     ]\n",
      "[442.97717 261.64423 712.70544   0.     ]\n",
      "[413.55228 286.74078 660.97845   0.     ]\n",
      "[ 969.9102  640.0315 1497.3713    0.    ]\n",
      "[522.91113 328.04462 821.67737   0.     ]\n",
      "Time since start: 4.03 min\n",
      "Trained from step 3000 to 3500 in 14.07 steps / sec\n",
      "Average discriminator output on Real: 364.83  Fake: 11.90\n"
     ]
    }
   ],
   "source": [
    "# Disable noisy output.\n",
    "tf.autograph.set_verbosity(0, False)\n",
    "\n",
    "import time\n",
    "steps_per_eval = 500 #@param\n",
    "max_train_steps = 5000 #@param\n",
    "batches_for_eval_metrics = 100 #@param\n",
    "\n",
    "# Used to track metrics.\n",
    "steps = []\n",
    "real_logits, fake_logits = [], []\n",
    "real_mnist_scores, mnist_scores, frechet_distances = [], [], []\n",
    "\n",
    "cur_step = 0\n",
    "start_time = time.time()\n",
    "while cur_step < max_train_steps:\n",
    "  \n",
    "    \n",
    "    \n",
    "  next_step = min(cur_step + steps_per_eval, max_train_steps)\n",
    "\n",
    "  start = time.time()\n",
    "  gan_estimator.train(input_fn, max_steps=next_step)\n",
    "  steps_taken = next_step - cur_step\n",
    "  time_taken = time.time() - start\n",
    "  print('Time since start: %.2f min' % ((time.time() - start_time) / 60.0))\n",
    "  print('Trained from step %i to %i in %.2f steps / sec' % (\n",
    "      cur_step, next_step, steps_taken / time_taken))\n",
    "  cur_step = next_step\n",
    "  \n",
    "  # Calculate some metrics.\n",
    "  metrics = gan_estimator.evaluate(input_fn, steps=batches_for_eval_metrics)\n",
    "  steps.append(cur_step)\n",
    "  real_logits.append(metrics['real_data_logits'])\n",
    "  fake_logits.append(metrics['gen_data_logits'])\n",
    "  print('Average discriminator output on Real: %.2f  Fake: %.2f' % (\n",
    "      real_logits[-1], fake_logits[-1]))\n",
    "  \n",
    "  # Print current predictions\n",
    "  iterator = gan_estimator.predict(\n",
    "      input_fn, hooks=[tf.train.StopAtStepHook(num_steps=21)])\n",
    "  try:\n",
    "    daily_prices = np.array([next(iterator) for _ in range(20)])\n",
    "  except StopIteration:\n",
    "    pass\n",
    "  for p in daily_prices:\n",
    "    print(p)\n",
    " \n",
    "  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
